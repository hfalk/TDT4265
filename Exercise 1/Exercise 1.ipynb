{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 1 - TDT 4265 Computer Vision and Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "The simplest form of a neural net is the perceptron, a binary classifier that maps an input to a binary output using a series of weights and biases. \n",
    "![Perceptron illustration](http://ataspinar.com/wp-content/uploads/2016/11/perceptron_schematic_overview.png \"Perceptron\")\n",
    "\n",
    "The perceptron function can be stated as $f(x) = \\begin{cases} 1\\text{ if }w^Tx + b > 0 \\\\ -1 \\text{ otherwise} \\end{cases}$. \n",
    "\n",
    "We can write this as a single multiplication by augmenting the weight and input vectors:\n",
    "$f(x) = \\begin{cases} 1\\text{  if }a^Ty > 0 \\\\ -1 \\text{ otherwise} \\end{cases}$ \n",
    "\n",
    "where a is the augmented weight vector combining the weights and biases, $a = [b, w]^T$, and y is the augmented input vector $y = [1, x]^T$\n",
    "\n",
    "For more information and a description of the learning algorithm see https://en.wikipedia.org/wiki/Perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Implement the perceptron algorithm. The skeleton code below can be modified freely and merely offers a starting suggestion. You can asssume all datapoints will be 2 dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-db88e1e8f4a1>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-db88e1e8f4a1>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    self.weights = #Implement weight initialization here.\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_size, ):\n",
    "        self.input_size = input_size\n",
    "        self.weights = #Implement weight initialization here.\n",
    "        \n",
    "    def classify(self, data):\n",
    "        #Implement perceptron classifier here\n",
    "        \n",
    "    def train_perceptron(self, data):\n",
    "        #Implement the perceptron learning algorithm here\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 2\n",
    "The code provided below generates a 2D linearly separable dataset. Use this to train a perceptron classifier and plot the data along with the separating hyperplane. We suggest using pyplot from the matplotlib library for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import rand\n",
    "import numpy as np\n",
    "\n",
    "def generateData(n):\n",
    "    \"\"\"\n",
    "    generates a 2D linearly separable dataset with n samples for each class.\n",
    "    Example usage:\n",
    "    trainingData = generateData(40)\n",
    "    x = trainingData[0] returns all the datapoints\n",
    "    y = trainingData[1] returns the corresponding labels\n",
    "    \"\"\"\n",
    "    xb = (rand(n)*2-1)/2-1\n",
    "    yb = (rand(n)*2-1)/2+0\n",
    "    xr = (rand(n)*2-1)/2+0\n",
    "    yr = (rand(n)*2-1)/2-1\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for i in range(len(xb)):\n",
    "        inputs.append([xb[i],yb[i],-1.0])\n",
    "        labels.append(1)\n",
    "        inputs.append([xr[i],yr[i],-1.0])\n",
    "        labels.append(-1)\n",
    "    return [np.array(inputs), np.array(labels)]\n",
    "\n",
    "# Insert code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "1. Can you find any issues with the way the perceptron algorithm finds a solution? Assume the dataset is linearly separable\n",
    "    \n",
    "2. When the dataset is linearly separable the perceptron algorithm will converge to a solution given enough iterations. However for non-separable data learning can fail completely and the algorithm might never converge. How would you modify the algorithm to ensure at least an approximate solution is found?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "### Task 1\n",
    "In this exercise you are going to get familiar with the basics of feedforward neural networks. Look at the code provided below and run it to figure out the accuracy of the network. Start by running the network for 2 epochs. When you are done, increase the number of epochs to 10, and finally run the network for 20 epochs. What are your findings? Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "import network\n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "training_data = list(training_data)\n",
    "validation_data = list(validation_data)\n",
    "\n",
    "net = network.Network([784, 10, 10])\n",
    "net.SGD(training_data, 10, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Modify the network to understand how its architecture affect its performance. Start by reducing the number of neurons in the hidden layer to 2, then make it 20. How does it affect the accuracy of the network? Train the network for 2, 10 and 20 epochs. When you are done with this, experiment with adding more hidden layers to the network. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "import network\n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "training_data = list(training_data)\n",
    "validation_data = list(validation_data)\n",
    "\n",
    "net = network.Network([784, 10, 10])\n",
    "net.SGD(training_data, 10, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Change the activation function of the network to a ReLU function in the file network.py. How does it affect the performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers\n",
    "Add your answers here.\n",
    "\n",
    "## Part 1\n",
    "\n",
    "#### Task 1\n",
    "\n",
    "#### Task 2\n",
    "\n",
    "#### Task 3\n",
    "\n",
    "## Part 2\n",
    "\n",
    "#### Task 1\n",
    "\n",
    "#### Task 2\n",
    "\n",
    "#### Task 3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
